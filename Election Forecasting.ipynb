{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data frames. \n",
    "df = pd.read_csv(\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\Pres_2.csv\") #Forecasting variables\n",
    "df_berry = pd.read_csv(\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\stateideology_v2018_2.csv\") #Berry's ideology scores\n",
    "df_pop = pd.read_csv(\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\population_data.csv\") #State population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to merge both population and berry ideology with electoral variables \n",
    "df = pd.merge(df, df_berry, how = 'outer') #Keep all observations\n",
    "df = pd.merge(df, df_pop, how = 'outer') #Keep all observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding regional, moralistic, and progressive history controls\n",
    "df_new = df[df['year'] > 1950]\n",
    "df_new['region_ne'] = 0\n",
    "df_new['region_mw'] = 0\n",
    "df_new['region_s'] = 0\n",
    "df_new['region_w'] = 0\n",
    "\n",
    "df_new['solidsouth'] = 0\n",
    "df_new['external'] = 0\n",
    "df_new['mountain'] = 0\n",
    "df_new['pacific'] = 0\n",
    "df_new['newengland'] = 0\n",
    "df_new['middleatlantic'] = 0\n",
    "df_new['eastnorth'] = 0\n",
    "df_new['westnorth'] = 0\n",
    "df_new['border'] = 0\n",
    "\n",
    "df_new['prog'] = 0\n",
    "df_new['trad'] = 0\n",
    "df_new['moral'] = 0\n",
    "df_new['ind'] = 0\n",
    "\n",
    "states = np.unique(df_new['statename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding regional controls in combination with the state names.\n",
    "df_new.loc[df_new['statename'] == 'Alabama', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Arkansas', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Delaware', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Dist. of Col.', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Florida', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Georgia', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Kentucky', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Louisiana', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Maryland', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Mississippi', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Missouri', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'North Carolina', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Oklahoma', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'South Carolina', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Tennessee', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Texas', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Virginia', 'region_s'] = 1\n",
    "df_new.loc[df_new['statename'] == 'West Virginia', 'region_s'] = 1\n",
    "\n",
    "df_new.loc[df_new['statename'] == 'Connecticut', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Maine', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Massachusetts', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'New Hampshire', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'New Jersey', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'New York', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Pennsylvania', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Rhode Island', 'region_ne'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Vermont', 'region_ne'] = 1\n",
    "\n",
    "df_new.loc[df_new['statename'] == 'Illinois', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Indiana', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Iowa', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Kansas', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Michigan', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Minnesota', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Nebraska', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'North Dakota', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Ohio', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'South Dakota', 'region_mw'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Wisconsin', 'region_mw'] = 1\n",
    "\n",
    "df_new.loc[df_new['statename'] == 'Alaska', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Arizona', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'California', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Colorado', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Hawaii', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Montana', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Idaho', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Nevada', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'New Mexico', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Oregon', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Utah', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Washington', 'region_w'] = 1\n",
    "df_new.loc[df_new['statename'] == 'Wyoming', 'region_w'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extending regional controls to 9 regions instead of 4.\n",
    "# df_new.loc[df_new['statename'] == 'Alabama', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Arkansas', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Delaware', 'middleatlantic'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Dist. of Col.', 'border'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Florida', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Georgia', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Kentucky', 'border'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Louisiana', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Maryland', 'border'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Mississippi', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Missouri', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'North Carolina', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Oklahoma', 'border'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'South Carolina', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Tennessee', 'border'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Texas', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Virginia', 'solidsouth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'West Virginia', 'border'] = 1\n",
    "\n",
    "# df_new.loc[df_new['statename'] == 'Connecticut', 'newengland'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Maine', 'newengland'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Massachusetts', 'newengland'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'New Hampshire', 'newengland'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'New Jersey', 'middleatlantic'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'New York', 'middleatlantic'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Pennsylvania', 'middleatlantic'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Rhode Island', 'newengland'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Vermont', 'newengland'] = 1\n",
    "\n",
    "# df_new.loc[df_new['statename'] == 'Illinois', 'eastnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Indiana', 'eastnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Iowa', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Kansas', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Michigan', 'eastnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Minnesota', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Nebraska', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'North Dakota', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Ohio', 'eastnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'South Dakota', 'westnorth'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Wisconsin', 'eastnorth'] = 1\n",
    "\n",
    "# df_new.loc[df_new['statename'] == 'Alaska', 'external'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Arizona', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'California', 'pacific'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Colorado', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Hawaii', 'external'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Montana', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Idaho', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Nevada', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'New Mexico', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Oregon', 'pacific'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Utah', 'mountain'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Washington', 'pacific'] = 1\n",
    "# df_new.loc[df_new['statename'] == 'Wyoming', 'mountain'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the index to begin at 0\n",
    "df_new.index = np.arange(0, len(df_new))\n",
    "df_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If we want to add state fixed effects\n",
    "# for i in range(len(df)):\n",
    "#     if df2.loc[i][\"statename\"]== 'Alabama':\n",
    "#         df2.iloc[[i],[31]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Alaska':\n",
    "#         df2.iloc[[i],[32]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Arizona':\n",
    "#         df2.iloc[[i],[33]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Arkansas':\n",
    "#         df2.iloc[[i],[34]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'California':\n",
    "#         df2.iloc[[i],[35]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Colorado':\n",
    "#         df2.iloc[[i],[36]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Connecticut':\n",
    "#         df2.iloc[[i],[37]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Delaware':\n",
    "#         df2.iloc[[i],[38]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Florida':\n",
    "#         df2.iloc[[i],[39]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Georgia':\n",
    "#         df2.iloc[[i],[40]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Hawaii':\n",
    "#         df2.iloc[[i],[41]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Idaho':\n",
    "#         df2.iloc[[i],[42]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Illinois':\n",
    "#         df2.iloc[[i],[43]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Indiana':\n",
    "#         df2.iloc[[i],[44]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Iowa':\n",
    "#         df2.iloc[[i],[45]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Kansas':\n",
    "#         df2.iloc[[i],[46]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Kentucky':\n",
    "#         df2.iloc[[i],[47]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Louisiana':\n",
    "#         df2.iloc[[i],[48]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Maine':\n",
    "#         df2.iloc[[i],[49]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Maryland':\n",
    "#         df2.iloc[[i],[50]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Massachusetts':\n",
    "#         df2.iloc[[i],[51]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Michigan':\n",
    "#         df2.iloc[[i],[52]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Minnesota':\n",
    "#         df2.iloc[[i],[53]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Mississippi':\n",
    "#         df2.iloc[[i],[54]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Missouri':\n",
    "#         df2.iloc[[i],[55]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Montana':\n",
    "#         df2.iloc[[i],[56]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Nebraska':\n",
    "#         df2.iloc[[i],[57]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Nevada':\n",
    "#         df2.iloc[[i],[58]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'New Hampshire':\n",
    "#         df2.iloc[[i],[59]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'New Jersey':\n",
    "#         df2.iloc[[i],[60]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'New Mexico':\n",
    "#         df2.iloc[[i],[61]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'New York':\n",
    "#         df2.iloc[[i],[62]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'North Carolina':\n",
    "#         df2.iloc[[i],[63]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'North Dakota':\n",
    "#         df2.iloc[[i],[64]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Ohio':\n",
    "#         df2.iloc[[i],[65]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Oklahoma':\n",
    "#         df2.iloc[[i],[66]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Oregon':\n",
    "#         df2.iloc[[i],[67]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Pennsylvania':\n",
    "#         df2.iloc[[i],[68]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Rhode Island':\n",
    "#         df2.iloc[[i],[69]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'South Carolina':\n",
    "#         df2.iloc[[i],[70]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'South Dakota':\n",
    "#         df2.iloc[[i],[71]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Tennessee':\n",
    "#         df2.iloc[[i],[72]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Texas':\n",
    "#         df2.iloc[[i],[73]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Utah':\n",
    "#         df2.iloc[[i],[74]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Vermont':\n",
    "#         df2.iloc[[i],[75]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Virginia':\n",
    "#         df2.iloc[[i],[76]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Washington':\n",
    "#         df2.iloc[[i],[77]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'West Virginia':\n",
    "#         df2.iloc[[i],[78]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Wisconsin':\n",
    "#         df2.iloc[[i],[79]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Wyoming':\n",
    "#         df2.iloc[[i],[80]] = 1\n",
    "#     elif df2.loc[i][\"statename\"]== 'Dist. of Col.':\n",
    "#         df2.iloc[[i],[81]] = 1\n",
    "#     else: \n",
    "#         df2.iloc[[i],[82]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to make sure state fiexed effects worked\n",
    "# for i in range(1051):\n",
    "#     if df2.loc[i][\"statename\"]== 'California':\n",
    "#         print(df2.loc[i][\"CA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Test to make sure state fiexed effects worked\n",
    "# print(df2[df2['test'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lagged varibles for the democratic vote percentage\n",
    "df_new['lag_dem_per'] = df_new.groupby('statename')['dem_per'].shift(1)\n",
    "df_new['lag2_dem_per'] = df_new.groupby('statename')['dem_per'].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Counter variable for year\n",
    "df_new['counter'] = df_new.groupby(['year']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interaction between South and Year\n",
    "df_new['int_south_year'] = df_new['region_s'] * df_new['counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if we want to cap the primary from 30-70%.\n",
    "df_new.loc[df_new['dem_primary'] > 70, 'dem_primary'] = 70\n",
    "df_new.loc[df_new['dem_primary'] < 30, 'dem_primary'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lagging the Berry State Ideology variables\n",
    "#Creating lagged varibles for the democratic vote percentage\n",
    "df_new['lag_citi'] = df_new.groupby('statename')['citi6016'].shift(1)\n",
    "df_new['lag_nom'] = df_new.groupby('statename')['inst6017_nom'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a csv file to see if we want to remove any columns.\n",
    "df_new.to_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\total.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the file again in case we adjust any columns in excel.\n",
    "df_new = pd.read_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\total.csv\")\n",
    "#df_new = pd.read_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our Training set (X) and our response variable (y)\n",
    "# cols = ['year', 'third_party', 'dem_inc', 'rep_inc', 'dem_inc_party', 'unem_dec_t_1', 'unem_per_change_t_2_t_1', 'inflation_t_1', 'last_recession', 'greatwar_years']\n",
    "#X = df2[cols]\n",
    "# X = df_new.iloc[:,[4, 9,10,12, 13, 14, 15, 16, 17, 18, 19, 23, 25,26,28,29,30,31,35,38,39,40,41,42,43,44,45,46,47,48,49,50,51]]\n",
    "X = df_new.dropna() # Dropping na values\n",
    "X =X[~X.isin([np.nan, np.inf, -np.inf]).any(1)] #This removes infinite and NaN values.\n",
    "X = X.reset_index() #Resetting the index\n",
    "y = X['dem_per'] #This is the response variable\n",
    "X = X.drop('dem_per', axis = 1) #Remove the response variable from our predictors.\n",
    "# X2 = df2.iloc[:, 31:81]\n",
    "\n",
    "# X = pd.concat([X, X2],  axis=1)\n",
    "\n",
    "# print(np.any(np.isnan(X)))\n",
    "# print(np.all(np.isfinite(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing new data to csv file\n",
    "df_new.to_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\test2.csv\", index = False)\n",
    "X.to_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\X.csv\", index = False)\n",
    "y.to_csv(r\"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\My Papers\\\\Papers\\\\Forecasting_2020\\\\y.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2 = y > 50\n",
    "# y2 = y2 * 1 #converting the booleans to integers\n",
    "# y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the response variable to work in the algorithm\n",
    "test = np.array(y_train)\n",
    "y_train = []\n",
    "for i in test:\n",
    "    y_train.append(i[0])\n",
    "\n",
    "test2 = np.array(y_test)\n",
    "y_test = []\n",
    "\n",
    "for i in test2:\n",
    "    y_test.append(i[0])\n",
    "    \n",
    "test3 = np.array(y)\n",
    "y = []\n",
    "\n",
    "for i in test3:\n",
    "    y.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Grid Search with Random Forest\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "#Best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = GridSearchCV(estimator = rf, param_grid = random_grid, cv = 5, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "#Best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False, True],\n",
    "    'max_depth': [90, 100, 110, 120],\n",
    "    'max_features': ['auto','sqrt'],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [1, 2, 3],\n",
    "    'n_estimators': [1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [70,80,90],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [2000,2100,2200]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [10,20,30,40,50,60,70],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [2050,2100,2150]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for the Random Forest\n",
    "rforest = RandomForestRegressor(bootstrap = False, max_depth= 30,max_features= 'sqrt', min_samples_leaf= 1,min_samples_split= 2, n_estimators= 2100)\n",
    "rforest.fit(X_train, y_train)\n",
    "scores = cross_val_score(rforest, X_train, y_train, cv=5, scoring = 'neg_root_mean_squared_error')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Machine\n",
    "# Hyperparameter tuning: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae\n",
    "\n",
    "#Random Grid Search with Gradient Boosting\n",
    "learning_rate = [0.01, .05, .1, .5, 1]\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "max_depth = np.linspace(1, 32, 32, endpoint=True)\n",
    "min_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "min_samples_leaf = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "max_features = list(range(1,X_train.shape[1]))\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'learning_rate': learning_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "gbm = GradientBoostingRegressor(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 100, random_state=42, cv = 5, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gbm_random.fit(X_train, y_train)\n",
    "\n",
    "#Best parameters\n",
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'learning_rate': [.05, .1, .15],\n",
    "    'max_depth': [20, 23, 27, 30, 33],\n",
    "    'max_features': [12,13,14,15,16],\n",
    "    'min_samples_leaf': [0.05, .1, .15],\n",
    "    'min_samples_split': [.3,.4,.5],\n",
    "    'n_estimators': [80,90,100,110,120]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = gbm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'learning_rate': [.05, .1,.15],\n",
    "    'max_depth': [16, 18, 20, 22, 24],\n",
    "    'max_features': [13, 14],\n",
    "    'min_samples_leaf': [.025, .05, .075],\n",
    "    'min_samples_split': [.2, .3, .4],\n",
    "    'n_estimators': [100, 110, 120]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = gbm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'learning_rate': [.1],\n",
    "    'max_depth': [12, 14, 16],\n",
    "    'max_features': [14],\n",
    "    'min_samples_leaf': [.05],\n",
    "    'min_samples_split': [.3],\n",
    "    'n_estimators': [105, 110]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = gbm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Random Search do Grid Search based on the results\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'learning_rate': [.1],\n",
    "    'max_depth': [2, 4, 6, 8, 10, 12],\n",
    "    'max_features': [14],\n",
    "    'min_samples_leaf': [.05],\n",
    "    'min_samples_split': [.3],\n",
    "    'n_estimators': [110]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = gbm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "#Training with the Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for the GBM\n",
    "gbm = GradientBoostingRegressor(learning_rate = .1, max_depth= 6,max_features= 14, min_samples_leaf= 0.05,min_samples_split= 0.3, n_estimators= 110, random_state = 42)\n",
    "gbm.fit(X_train, y_train)\n",
    "scores = cross_val_score(gbm, X_train, y_train, cv=5, scoring = 'neg_root_mean_squared_error')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(learning_rate = .1, max_depth= 6,max_features= 14, min_samples_leaf= 0.05,min_samples_split= 0.3, n_estimators= 110, random_state = 42)\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "gbm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an array to calculate the testing accuracy.\n",
    "y_test = y_test.reshape(180, 1) #reshaping the data to be the length of the test set X 1\n",
    "y_pred = y_pred.reshape(180, 1) #reshaping the data to be the length of the prediction array X 1\n",
    "results = np.concatenate((y_test, y_pred), axis = 1) #Combinng the two arrays [[Y,Y_hat],[Y,Y_hat],...]\n",
    "\n",
    "#Add 1 for every instance where both the prediction and training response variable are either above or below 50 percent\n",
    "results2 = 0\n",
    "for i in range(180):\n",
    "    if results[i][0] > 50 and results[i][1] > 50:\n",
    "        results2 += 1\n",
    "    elif results[i][0] < 50 and results[i][1] < 50:\n",
    "        results2 += 1\n",
    "    else:\n",
    "        results2 += 0\n",
    "\n",
    "#Calculaing the accuray\n",
    "results2/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(learning_rate = .1, max_depth= 6,max_features= 14, min_samples_leaf= 0.05,min_samples_split= 0.3, n_estimators= 110, random_state = 42)\n",
    "gbm.fit(X, y)\n",
    "y_pred = gbm.predict(X)\n",
    "gbm.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an array to calculate the accuracy of the final model on all of the data.\n",
    "y = y.reshape(600, 1) #reshaping the data to be the length of the test set X 1\n",
    "y_pred = y_pred.reshape(600, 1) #reshaping the data to be the length of the prediction array X 1\n",
    "results = np.concatenate((y, y_pred), axis = 1) #Combinng the two arrays [[Y,Y_hat],[Y,Y_hat],...]\n",
    "\n",
    "#Add 1 for every instance where both the prediction and training response variable are either above or below 50 percent\n",
    "results2 = 0\n",
    "for i in range(600):\n",
    "    if results[i][0] > 50 and results[i][1] > 50:\n",
    "        results2 += 1\n",
    "    elif results[i][0] < 50 and results[i][1] < 50:\n",
    "        results2 += 1\n",
    "    else:\n",
    "        results2 += 0\n",
    "\n",
    "#Calculaing the accuray\n",
    "results2/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
